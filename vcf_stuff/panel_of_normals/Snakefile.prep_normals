# Snakemake file for preparing PoN filtering

import os
import glob
import re
from ngs_utils.file_utils import get_ungz_gz, splitext_plus, add_suffix

from python_utils import hpc
from vcf_stuff.panel_of_normals import get_normals_yaml_path
import yaml
from cyvcf2 import VCF, Writer


def str_to_lua_variable_name(name):
    name = os.path.basename(name)
    name = name.replace('.vcf.gz', '')
    name = re.sub('[^0-9a-zA-Z_]', '_', name) # Remove invalid characters
    name = re.sub('^[^a-zA-Z_]+', '_', name)  # Remove leading characters until we find a letter or underscore
    return name


do_merge_multiallelic = True  # True if want to compare SITE, False if ALT


PON_FILE = f'panel_of_normals.vcf.gz'


data = dict()
if hpc.get_loc().name == 'spartan':
    with open(get_normals_yaml_path()) as f:
        data = yaml.load(f)

if hpc.get_loc().name == 'vlad':
    data = {'tmp': '/Users/vsaveliev/Analysis/panel_of_normals/GRCh37/normals/MH17B001P004-germline-ensemble-annotated.vcf.gz'}

if 'normal' in config:
    data = dict([config['normal'].split(':')])

cohort_by_sample = dict()
vcf_by_sample = dict()

for entry in data:
    if isinstance(entry, str):  # VCF path
        sname = VCF(entry).samples[0]
        cohort_by_sample[sname] = sname
        vcf_by_sample[sname] = entry
    else:  # single-item dict {cohort: [VCF paths]}
        cohort_name, vcf_paths = list(entry.items())[0]
        for vcf_path in vcf_paths:
            sname = VCF(vcf_path).samples[0]
            cohort_by_sample[sname] = cohort_name
            vcf_by_sample[sname] = vcf_path


rule all:
    input:
        add_suffix(PON_FILE, 'snps'),
        add_suffix(PON_FILE, 'indels')


rule clean_vcf:
    input:
        vcf = lambda wc: vcf_by_sample[wc.sample],
        tbi = lambda wc: vcf_by_sample[wc.sample] + '.tbi'
    output:
        vcf = 'work/clean/{sample}.clean.vcf.gz',
        tbi = 'work/clean/{sample}.clean.vcf.gz.tbi'
    shell:
        'bcftools annotate -x INFO,FORMAT {input.vcf} -Oz -o {output.vcf} && tabix -p vcf {output.vcf}'


rule normalise_vcf:
    input:
        rules.clean_vcf.output.vcf
    output:
        vcf = 'work/clean/{sample}.clean.norm.vcf.gz',
        tbi = 'work/clean/{sample}.clean.norm.vcf.gz.tbi'
    shell:
        'norm_vcf {input} -o {output.vcf}'


# TODO:
# Try: take PCGR `<batch>-??????-normal.pcgr.snvs_indels.tiers.tsv`, remove TIER1 and TIER2


# Merge VCFs, make multiallelic indels as we will ignore ALT for indels
rule combine_vcfs:
    input:
        expand(rules.normalise_vcf.output.vcf, sample=vcf_by_sample.keys()),
    output:
        vcf = 'work/clean_merged.vcf.gz',
        tbi = 'work/clean_merged.vcf.gz.tbi'
    threads: 32
    shell:
        'bcftools merge -m indels {input} --threads {threads} -Oz -o {output.vcf} && tabix -p vcf {output.vcf}'
        # 'bcftools concat {input} -d all --allow-overlaps --threads {threads} -Oz -o {output}'


rule count_hits:
    input:
        rules.combine_vcfs.output[0]
    output:
        get_ungz_gz(PON_FILE)[0]
    run:
        vcf = VCF(input[0])
        vcf.add_info_to_header({'ID': 'PoN_cohorts', 'Description': 'Panel of normal hits', 'Type': 'Integer', 'Number': '1'})
        vcf.add_info_to_header({'ID': 'PoN_samples', 'Description': 'Panel of normal hits', 'Type': 'Integer', 'Number': '1'})
        w = Writer(output[0], vcf)
        failed_w = open(splitext_plus(output[0])[0] + '_failed_lines.tsv', 'w')

        failed_lines = 0
        good_lines = 0
        for v in vcf:
            try:
                v_samples = [s for s, gt in zip(vcf.samples, v.format('GT')) if gt]
            except:
                failed_lines += 1
                failed_w.write(str(v))
            else:
                good_lines += 1
                cohorts = set(cohort_by_sample[s] for s in v_samples)
                v.INFO['PoN_cohorts'] = len(cohorts)
                v.INFO['PoN_samples'] = len(v_samples)
                w.write_record(v)
        print(f'Written {good_lines} lines, failed {failed_lines} lines')
        w.close()
        failed_w.close()
        vcf.close()


rule bgzip_and_tabix_final_vcf:
    input:
        rules.count_hits.output[0]
    output:
        vcf = PON_FILE,
        tbi = PON_FILE + '.tbi'
    shell:
        'bgzip {input} && tabix -p vcf {output.vcf}'


rule split_snps_indels:
    input:
        rules.bgzip_and_tabix_final_vcf.output[0]
    output:
        snps_vcf = add_suffix(PON_FILE, 'snps'),
        inds_vcf = add_suffix(PON_FILE, 'indels'),
        snps_tbi = add_suffix(PON_FILE, 'snps') + '.tbi',
        inds_tbi = add_suffix(PON_FILE, 'indels') + '.tbi'
    shell:
        'bcftools view -v snps {input} -Oz -o {output.snps_vcf} && tabix -p vcf {output.snps_vcf} && '
        'bcftools view -V snps {input} -Oz -o {output.inds_vcf} && tabix -p vcf {output.inds_vcf}'






















